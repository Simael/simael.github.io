---
title: "CHAOS: Chart Analysis with Outlier Samples"
collection: publications
category: conferences
permalink: /publication/2025-05-22_moured2025chaos
excerpt: 'The paper introduces CHAOS, a robustness benchmark for testing MLLMs on charts with textual and visual perturbations at varying severity. It evaluates 13 models across ChartQA and Chart-to-Text tasks, revealing weaknesses and guiding future research in chart understanding. Data and code are publicly released.'
date: 2025-05-22
venue: 'arXiv, technical report'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/abs/2505.17235'
bibtexurl: 'http://simael.github.io/files/2025-05-22_moured2025chaos.bib'
authors: 'Omar Moured, Yufan Chen, Ruiping Liu, Simon Rei√ü, Philip Torr, Jiaming Zhang, Rainer Stiefelhagen.'
---
Charts play a critical role in data analysis and visualization, yet real-world applications often present charts with challenging or noisy features. However, "outlier charts" pose a substantial challenge even for Multimodal Large Language Models (MLLMs), which can struggle to interpret perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier Samples), a robustness benchmark to systematically evaluate MLLMs against chart perturbations. CHAOS encompasses five types of textual and ten types of visual perturbations, each presented at three levels of severity (easy, mid, hard) inspired by the study result of human evaluation. The benchmark includes 13 state-of-the-art MLLMs divided into three groups (i.e., general-, document-, and chart-specific models) according to the training scope and data. Comprehensive analysis involves two downstream tasks (ChartQA and Chart-to-Text). Extensive experiments and case studies highlight critical insights into robustness of models across chart perturbations, aiming to guide future research in chart understanding domain. Data and code are publicly available at: [this http URL](https://huggingface.co/datasets/omoured/CHAOS). 