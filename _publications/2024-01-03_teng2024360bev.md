---
title: "360bev: Panoramic semantic mapping for indoor bird's-eye view"
collection: publications
category: conferences
permalink: /publication/2024-01-03_teng2024360bev
excerpt: 'This paper introduces the 360BEV task-mapping panoramic images with depth to bird’s-eye-view semantic maps for holistic indoor scene representation. It releases two datasets (360BEV-Matterport and 360BEV-Stanford) and proposes 360Mapper, a method that outperforms prior approaches by +7.60% and +9.70% mIoU on the respective datasets.'
date: 2024-01-03
venue: 'WACV'
paperurl: 'https://openaccess.thecvf.com/content/WACV2024/html/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.html'
bibtexurl: 'http://simael.github.io/files/2024-01-03_teng2024360bev.bib'
authors: 'Zhifeng Teng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Hao Shi, Simon Reiß, Ke Cao, Rainer Stiefelhagen.'
---
Seeing only a tiny part of the whole is not knowing the full circumstance. Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps from egocentric views, is restricted when using a narrow Field of View (FoV) alone. In this work, mapping from 360deg panoramas to BEV semantics, the 360BEV task, is established for the first time to achieve holistic representations of indoor scenes in a top-down view. Instead of relying on narrow-FoV image sequences, a panoramic image with depth information is sufficient to generate a holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets, 360BEV-Matterport and 360BEV-Stanford, both of which include egocentric panoramic images and semantic segmentation labels, as well as allocentric semantic maps. Besides delving deep into different mapping paradigms, we propose a dedicated solution for panoramic semantic mapping, namely 360Mapper. Through extensive experiments, our methods achieve 44.32% and 45.78% mIoU on both datasets respectively, surpassing previous counterparts with gains of+ 7.60% and+ 9.70% in mIoU.