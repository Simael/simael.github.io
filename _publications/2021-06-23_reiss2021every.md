---
title: "Every annotation counts: Multi-label deep supervision for medical image segmentation"
collection: publications
category: conferences
permalink: /publication/2021-06-23_reiss2021every
excerpt: 'We propose a semi-weakly supervised segmentation method using a student-teacher model and multi-label deep supervision. Our approach integrates various annotation types - full, bounding boxes, global labels, or none - reducing expensive labels by 94.22% while achieving segmentation close to fully supervised models. Validated on retinal fluid segmentation, it narrows the gap to the best baseline to just 5% mean IoU.'
date: 2021-06-23
venue: 'CVPR'
paperurl: 'https://openaccess.thecvf.com/content/CVPR2021/html/Reiss_Every_Annotation_Counts_Multi-Label_Deep_Supervision_for_Medical_Image_Segmentation_CVPR_2021_paper.html'
bibtexurl: 'http://simael.github.io/files/2021-06-23_reiss2021every.bib'
authors: 'Simon Rei√ü, Constantin Seibold, Alexander Freytag, Erik Rodner, Rainer Stiefelhagen.'
---
 Pixel-wise segmentation is one of the most data and annotation hungry tasks in our field. Providing representative and accurate annotations is often mission-critical especially for challenging medical applications. In this paper, we propose a semi-weakly supervised segmentation algorithm to overcome this barrier. Our approach is based on a new formulation of deep supervision and student-teacher model and allows for easy integration of different supervision signals. In contrast to previous work, we show that care has to be taken how deep supervision is integrated in lower layers and we present multi-label deep supervision as the most important secret ingredient for success. With our novel training regime for segmentation that flexibly makes use of images that are either fully labeled, marked with bounding boxes, just global labels, or not at all, we are able to cut the requirement for expensive labels by 94.22% - narrowing the gap to the best fully supervised baseline to only 5% mean IoU. Our approach is validated by extensive experiments on retinal fluid segmentation and we provide an in-depth analysis of the anticipated effect each annotation type can have in boosting segmentation performance.