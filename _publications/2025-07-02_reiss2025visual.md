---
title: "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?"
collection: publications
category: conferences
permalink: /publication/2025-07-02_reiss2025visual
excerpt: 'This paper explores visual in-context learning for adapting a single model to multi-step vision tasks without retraining. It introduces a synthetic task generation engine from segmentation data, studies codebooks and masking-based training.'
date: 2025-07-02
venue: 'to appear at ICCV'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/abs/2507.00868'
bibtexurl: 'http://simael.github.io/files/2025-07-02_reiss2025visual.bib'
authors: 'Simon Rei√ü, Zdravko Marinov, Alexander Jaus, Constantin Seibold, M. Saquib Sarfraz, Erik Rodner, Rainer Stiefelhagen.'
---
In this paper, we explore the potential of visual in-context learning to enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training. Unlike previous approaches, our focus is on training in-context learners to adapt to sequences of tasks, rather than individual tasks. Our goal is to solve complex tasks that involve multiple intermediate steps using a single model, allowing users to define entire vision pipelines flexibly at test time. To achieve this, we first examine the properties and limitations of visual in-context learning architectures, with a particular focus on the role of codebooks. We then introduce a novel method for training in-context learners using a synthetic compositional task generation engine. This engine bootstraps task sequences from arbitrary segmentation datasets, enabling the training of visual in-context learners for compositional tasks. Additionally, we investigate different masking-based training objectives to gather insights into how to train models better for solving complex, compositional tasks. Our exploration not only provides important insights especially for multi-modal medical task sequences but also highlights challenges that need to be addressed. 